---
title: "R Notebook"
output: html_notebook
---

```{r}
library(prob)
library(tidyverse)
```

## Lesson 1

Distributions - measures of centre

definition
- all the possible outcome of a random variable, with the corresponding frequency
(freq distribution), or prob values (prob distr)

distr tell us how common/likely various outcomes are

```{r}
s_three_coins <- tosscoin(3)
s_three_coins
```

Distribution 1 : uniform distribution
```{r}
s_three_coins_p <- s_three_coins %>%
  mutate(label = str_c(toss1, toss2, toss3)) %>%
  mutate(prob = 1/n())

s_three_coins_p %>%
  ggplot() +
  geom_col(aes(x = label, y = prob))
```
Uniform distribution 
all the bars are the same height, indicating each probability is the same
(same likelihood of occurring)


What are the probabilities of getting 0, 1, 2, heads, on 3 unbiased coins

```{r}
s_three_coins_p %>%
  mutate(n_heads = str_count(label, "H")) %>%
  group_by(n_heads) %>%
  summarise(prob = sum(prob)) %>%
  ggplot()+
  geom_col(aes(x = n_heads, y = prob))
```
distribution is no longer uniform, but it is still discrete.
the bars have differing heights.


General properties of discrete distributions
x-axis = outcomes
y-axis = probabilitiy / freq
THE PROB MUST SUM TO 1

If you stacked all the bars on top of each other, the y axis would reach one



Measures of centrality - mean, median and mode


```{r}
# loading in data
library(janitor)
library(lubridate)

air_con_data <- read.csv("AirConSales.csv")
air_con_data <- clean_names(air_con_data)

# wrangle date column
air_con_data <- air_con_data %>%
  mutate(date = mdy(date))

```


We've got measurements for each day, but we want to look at how likely it will be
to sell a certain of number of units (air cons)


Generating a freq table

```{r}
sales_freq_table <- air_con_data %>%
  tabyl(units_sold)
```

```{r}
# generate freq distr from freq table
sales_freq_table %>%
  ggplot() +
  geom_col(aes(x = units_sold, y = n))
```


`mean` = sum of all values /n of all values
`median` = 50% of data beneath, 50% of data above
`mode` = most frequently occurring value

R has a mean and median function but not a mode function

```{r}
# generates mode
get_mode <- function(data){

  tabled_data <- table(data)
  table_names <- names(tabled_data)
  
  return(as.numeric(table_names[tabled_data == max(tabled_data) ]))
  
}
```

Let's calculate these values for our distribution

```{r}
air_con_data %>%
  summarise(mean_daily_sales = mean(units_sold),
            median_daily_sales = median(units_sold),
            mode_daily_sales = get_mode(units_sold))
```

The mean is more heavily/easily skewed by extremes, aka the bill gates effect


Measures of centre and outliers
If you suspect or observe outliers, be careful which measure of centre you use.
Always look at the shape/distr of your data.


Modality
- number of peaks in the distribution

Unimodal - one
Bimodal - two


Skew / Skewness / Asymmetry

Left-skew = left-tailed - typically falls mean < median < mode
Right-skew = right-tailed - typically falls mean > median > mode


Calculating skew

```{r}
library(e1071)
```

```{r}
left_skewed <- read_csv("leftskew.csv")
right_skewed <- read_csv("rightskew.csv")

left_skewed %>%
  summarise(skewness = skewness(x, type = 1))

right_skewed %>%
  summarise(skewness = skewness(x, type = 1))
```

left-skew = ~-1
right-skew = ~0.7

direction - positive/negative
negative skew = left skew
positive skew = right skew

magnitude - 0-0.5 = fairly symmetrical
            0.5 -1 = moderately skewed
            '> 1' = heavily skewed


```{r}
skew_air_con <- air_con_data %>%
  summarise(skewness = skewness(units_sold, type = 1))

# heavily skewed to the right
```

Key takeaway

**It's important to visualise your data**



## Lesson 2 

### Distributions - Measures of spread

Range

From the lowest value to the highest
range = max_value - min value

```{r}
jobs <- read.csv("TyrellCorpJobs.csv")
jobs <- clean_names(jobs)
```

range of salaries if the max - the min

```{r}
jobs %>%
  summarise(max = max(salary),
            min = min(salary),
            range = max(salary) - min(salary))
# note - R has a range function, but it returns the max and min values
```


```{r}
# positions together

jobs %>%
  ggplot() +
  geom_histogram(aes(x = salary), col = "white", bins = 25)
```

```{r}
# seperating positions
jobs %>%
  ggplot() +
  geom_histogram(aes(x = salary), col = "white", bins = 25) +
  facet_wrap(~position)
```

```{r}
jobs %>%
  group_by(position) %>%
  summarise(range = diff(range(salary)))
```

A potential problem with using the range as a measure of spread - is is very
strongly affected by outliers


Quartiles and interquartile ranges

while quartile / quarter means 4, quartiles are the box/plots inbetween the
median and 25 / median and 75

Q1 - the value splitting the distribution such that 25% of the data is below that
line and 75% above 
Q2 - " " " such that 50% is below and 50% (median)
Q3 - " " " such that 75% of the data is below that
line and 25% above
```{r}
# quartiles for the jobs data
jobs %>%
  group_by(position) %>%
  summarise(
    Q1 = quantile(salary, 0.25),
    Q2 = quantile(salary, 0.5),
    Q3 = quantile(salary, 0.75)
  )
```

```{r}
# quartiles on the vis - code from david
iqr <- jobs %>% 
  group_by(position) %>% 
  summarise(
    Q1 = quantile(salary, 0.25),
    Q2 = quantile(salary, 0.5),
    Q3 = quantile(salary, 0.75)
  ) %>% 
  pivot_longer(Q1:Q3, names_to = "fencepost", values_to = "value")

jobs %>% 
  ggplot() +
  geom_histogram(aes(x = salary), col = "white", bins = 25) +
  geom_vline(xintercept = iqr$value, linetype = "dashed", colour = "red") +
  facet_wrap(~position)
```

## The interquartile range

the range between fence posts q3 and q1

```{r}
jobs %>%
  group_by(position) %>%
  summarise(
    Q1 = quantile(salary, 0.25),
    Q2 = quantile(salary, 0.5),
    Q3 = quantile(salary, 0.75),
    IQR = Q3 - Q1,
    # r also has an ICR function:
    IQR_fun = IQR(salary)
  )
```


# The five figure summary

min, Q1, Q2, Q3, max

```{r}
# you can get this by using the summary functino
summary(jobs$salary)
```

We wanted accounting and management to be grouped


```{r}
library(skimr)
```

`skim()` is a more sophisticated summary

```{r}
jobs %>%
  select(-x) %>%
  group_by(position) %>%
  skim()
```

## Box plots

An excellent statistical diagnostic tool

They can have many names; box plots, hinge plots, box and whisker plot, tukey plots


```{r}
jobs %>%
  ggplot() +
  aes(x = salary, y = position) +
  stat_boxplot(geom = "errorbar", width = 0.5) +
  geom_boxplot()
  
```

whiskers are typically positioned 1.5 IQRs above and below Q1 and Q3 respectively.
any points that lie outside of these are typically defined as outliers.

## Skew in boxplots 

What does skew look like in a box plot?


```{r}
heavily_rskewed <- read_csv("heavily_right_skewed.csv")
```

```{r}
heavily_rskewed %>%
  ggplot() +
  geom_boxplot(aes(x = x))
```


```{r}
heavily_rskewed %>%
  ggplot() +
  geom_histogram(aes(x = x), colour = "white", bins = 30)
```

task
```{r}
jobs %>%
  group_by(position) %>%
  summarise(skewness = skewness(salary, type = 1))

positive = right skew
'0.5' - '1' = moderate

```

- aside
removing outliers from the data (potentially dangerous)
- get where Q1 and Q3 are
- get IQ range
- calculate distance to the whisker(1.5*IQR)
- filter values above Q3 + 1.5IQR and below Q1 - 1.5IQR



## Variance

- a single measure of spread
how far each value is from the mean

How to calculate variance step by step
1. get the difference between each value and mean squared
2. sum them
3. divide by the total number of values - 1

In R - use the var function

```{r}
jobs %>%
  group_by(position) %>%
  summarise(variance = var(salary))
```

this is dollars squared, which doesn't really mean a lot to me

## Standard Deviation

- a more interpretable single measure spread of the data

to go from variance to SD, all we do is take the srqt

```{r}
jobs %>%
  group_by(position) %>%
  summarise(st_dev = sqrt(var(salary)),
            sd = sd(salary))
  
```

so... if you come and work for TyrellCorp in Account, you can expect to earn 34,115 +/- 2,400
In management, you can expect to earn 51000 +/- 11,000


# Common distributions

## discrete vscontinuous variables

- all categorical variables are discrete (e.g strings)

- numerical variables can be discrete or continuous
- discrete are:
- continuous are: 

Probability mass function
For discrete variables:
- bars / each prob must add up to 1 together
- each bar individually must not be over 1

For continuous variables:
- probability density;
  considering an area of the line/area, and then a subset of that
  
  ??
  
## Discrete uniform distribution
- there is equal probability for each outcome


## Cumulative distribution function

```{r}
dice <- tibble(
    x = 1:6,
    f_x = replicate(6, 1/6)
  ) %>%
  mutate(F_x = cumsum(f_x))

dice %>%
  ggplot(aes(x = x, y = F_x)) +
  geom_step() +
  xlab("number rolled on die") + ylab("probability") +
  scale_x_continuous(breaks = 1:6) +
  scale_y_continuous(breaks = seq(0, 1, 0.1)) +
  ylim(0, 1) 
```


## Continuous distribution
something about cumulative density?
- return to notes, something about functions in r?
d(something) etc

## Normal distribution


```{r}
three_norms <- tibble(
  x =  seq(0, 20, 0.1),
  f1_x = dnorm(x = x, mean = 10, sd = 1), 
  f2_x = dnorm(x = x, mean = 10, sd = 2),
  f3_x = dnorm(x = x, mean = 10, sd = 3)
)
three_norms %>%
  ggplot() +
  geom_line(aes(x = x, y = f2_x), col = "black")


three_norms %>%
  ggplot() +
  geom_line(aes(x = x, y = f1_x), col = "black") +
  geom_line(aes(x = x, y = f2_x), col = "red") +
  geom_line(aes(x = x, y = f3_x), col = "blue")
```

Although the tails do not hit 0, they do not reach 0 (the probability of getting that value becomes increasingly small)

## " p "
- qbinom and CDF are opposite functions / correspond to each other

## Standard normal, aka a z transform / z distribution

## Standard normal distribution
- has the mean set to 0 (standardised), and has the SD set to 1


```{r}
z = seq(from = -4, to = 4, by = 0.01)
f_z = dnorm(x = z)
# calculates the density of the distribution
```










