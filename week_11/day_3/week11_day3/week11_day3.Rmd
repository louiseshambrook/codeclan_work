---
title: "R Notebook"
output: html_notebook
---

Week 11 Day 3

### Lesson 1
## Clustering

Clustering is unsupervised

Data can be segmented, manually - groups can be split manually into groups, e.g.

age over 20 + has 4+ children, age under 20 + has <4 children

This can be easy for small datasets, but can quickly become complicated

This is clustering

Clustering algorithims
- connectivity models
- hierarchical clustering
- centroid models
- k means clustering
- distribution models
- density models


### Lesson 2

## Hierarchical Clustering 

When there is an inherent hierarchy in the data

There are two ways to do hierarchical clustering;
aggolomerative
divsive

They both end up in a dendrogram.

```{r}
library(tidyverse)
library(cluster)
library(factoextra)
library(dendextend)
library(corrplot)
```

```{r}
edu_data <- read_csv("data/school_data.csv")
```

```{r}
edu_data <- edu_data %>%
  column_to_rownames("X1")
```


```{r}
# scaling the dataframe as the state_school ranges from 0 to 30/40k, and the home_school is significantly lower
edu_data_scale <- edu_data %>%
  mutate_if(is.numeric, scale)

edu_data_scale <- edu_data %>%
  mutate(across(where(is.numeric), scale))
```

```{r}
# checking that the dataframe has been normalised

edu_data_scale %>%
  pivot_longer(cols = everything(),
               names_to = "type", 
               values_to = "value") %>% #convert data to long format
  group_by(type) %>%
  summarise(mean = round(mean(value)), 
            sd = sd(value))
```

```{r}
# creating a correlation plot - like we did with the penguins

corrplot(cor(edu_data_scale), method = "number", type = "lower")
```

Calculating the distances between variables
```{r}
diss_matrix <- edu_data_scale %>%
  select(home_school) %>%
  dist(method = "euclidean")

```

```{r}
# visualising the above matrix / distance calculations
fviz_dist(diss_matrix)
```

building the dendrogram
```{r}
clusters <- diss_matrix %>%
  hclust(method = "complete")
```

Plotting the dendrogram
```{r}
clusters %>%
  plot(cex = 0.5, hang = -5)
```

calculating alternative clusters
```{r}
alt_clusters <- diss_matrix %>%
  hclust(method = "ward.D2")
```

```{r}
clustering_dendrogram <- clusters %>%
  as.dendrogram() %>%
  dendextend::set("labels_cex", 0.5)

clustering_dendrogram
```

```{r}
alt_clustering_dendrogram <- alt_clusters %>%
  as.dendrogram() %>%
  dendextend::set("labels_cex", 0.6)

alt_clustering_dendrogram
```

```{r}
dend_diff(clustering_dendrogram, alt_clustering_dendrogram)
```

How many clusters do we want / can we identify for our dataset?
```{r}
# creating our dendrogram

plot(clusters, cex = 0.6, hang = -1)
```

```{r}
plot(clusters, cex = 0.6, hang = -1)
rect.hclust(clusters, k = 2, border = 2:5) # this marks the significant areas / clusters and can only be run with the plot code
# K = 2; number of clusters you want
```

```{r}
plot(clusters, cex = 0.6, hang = -1)
rect.hclust(clusters, k = 4, border = 2:5) 
```

```{r}
edu_clustered_h2 <- edu_data %>%
  mutate(school_cluster = cutree(clusters, 2))
```

Carry out cluster analysis for other variables. 
What do you find for each variable?
How many clusters do you choose, and why?
What are your interpretations fromr your analysis?

```{r}
# chosen variable - private_school
# calculating the distance between variables

diss_matrix_ps <- edu_data_scale %>%
  select(private_school) %>%
  dist(method = "euclidean")

# visualising it
fviz_dist(diss_matrix_ps)

# building the dendrogram
dendro_ps <- diss_matrix_ps %>%
  hclust(method = "complete")

# plotting this
dendro_ps %>%
  plot(cex = 0.5, hang = -5)

# marking significant areas of plot
plot(dendro_ps, cex = 0.5, hang = -5)
rect.hclust(dendro_ps, k = 6, border = 2:5)


```

** LUNCH BREAK** 


## K means clustering

You can tell it how many you're looking for, and it will group into this many (centroids)

When running k-means, the centroids get randomly placed
Then observations get assigned to a centroid, based on euclidean distance
The centroid is then moved to the centre of the cluster


```{r}
edu_data <- read_csv("data/school_data.csv")
```
```{r}
library(janitor)

edu_data <- edu_data %>%
  column_to_rownames("X1") %>%
  clean_names()
```

```{r}
edu_data <- edu_data %>%
  select(home_school, state_school)
```

plotting the data, getting an overview of it
```{r}
edu_data %>%
  ggplot(aes(x = home_school, y = state_school)) +
  geom_point()
```

Scaling the data
```{r}
edu_scale <- edu_data %>%
  mutate_if(is.numeric, scale)

```

Doing the k-means clustering
```{r}
set.seed(1234)

# when doing this in practice, the seed shouldn't be set. 

clustered_edu <- kmeans(edu_scale,
                        centers = 6,
                        nstart = 25)
clustered_edu
```

```{r}
library(tidyverse)
library(broom)

tidy(clustered_edu,
     col.names = colnames(edu_scale))

augment(clustered_edu, edu_data)
```

```{r}
clustered_edu$totss
```


```{r}

max_k <- 20

k_clusters <- tibble(k = 1:max_k) %>%
  mutate(
    kclust = map(k, ~ kmeans(edu_scale, .x, nstart = 25)),
    tidied = map(kclust, tidy),
    glanced = map(kclust, glance),
    augmented = map(kclust, augment, edu_data)
  )

k_clusters
```

How to access / read the data
```{r}
k_clusters %>%
  select(1,5) %>%
  unnest()
```


metric of interest: __tot.withinss__

optimal number of clusters

```{r}
# elbow method/point - is a way of choosing the number of clusters


```

```{r}
clusterings <- k_clusters %>%
  unnest(glanced)
```

```{r}
ggplot(clusterings, aes(x=k, y=tot.withinss)) +
  geom_point() +
    geom_line() +
    scale_x_continuous(breaks = seq(1, 20, by = 1))
```

```{r}
library(factoextra)

fviz_nbclust(edu_scale,
             kmeans,
             method = "wss",
             nstart = 25)
```

```{r}
fviz_nbclust(edu_scale, 
             kmeans, 
             method = "silhouette", 
             nstart = 25)
```

a different method of chosing the optimal number of clusters
```{r}
fviz_nbclust(edu_scale, 
             kmeans, 
             method = "gap_stat", 
             nstart = 25, 
             k.max = 10)
```

visualising the chosen plot
```{r}
clusterings %>% 
  unnest(cols = c(augmented)) %>%
  filter(k <= 6) %>%
  ggplot(aes(x = home_school, y = state_school)) +
  geom_point(aes(color = .cluster))
```

plotting 2 clusters, with labels
```{r}
clusterings %>% 
  unnest(cols = c(augmented)) %>%
  filter(k == 2) %>%
 ggplot(aes(x = home_school, y = state_school, colour = .cluster, label = .rownames)) +
  geom_point(aes(color = .cluster)) +
  geom_text(hjust = 0, vjust = - 0.5, size = 3)
```










